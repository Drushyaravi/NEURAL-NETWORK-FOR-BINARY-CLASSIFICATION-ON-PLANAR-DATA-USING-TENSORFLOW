PYTHON CODE USING TENSORFLOW:
==============================

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf

# Generate synthetic planar data
np.random.seed(1)
X = np.random.randn(2, 400)
Y = np.squeeze((X[0, :] ** 2 + X[1, :] ** 2) < 1.0).astype(int)

# Visualize the data
plt.scatter(X[0, :], X[1, :], c=Y, cmap=plt.cm.Spectral)
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Planar Data')
plt.show()

# Define the neural network architecture
def create_nn_model(input_shape, hidden_units):
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(hidden_units, activation='tanh', input_shape=input_shape),
        tf.keras.layers.Dense(1, activation='sigmoid')
    ])
    return model

# Initialize parameters
def initialize_parameters(input_shape, hidden_units):
    tf.random.set_seed(1)
    W1 = tf.Variable(tf.random.normal([hidden_units, input_shape[0]], stddev=0.1), name='W1')
    b1 = tf.Variable(tf.zeros([hidden_units, 1]), name='b1')
    W2 = tf.Variable(tf.random.normal([1, hidden_units], stddev=0.1), name='W2')
    b2 = tf.Variable(tf.zeros([1, 1]), name='b2')
    return {"W1": W1, "b1": b1, "W2": W2, "b2": b2}

# Forward propagation
def forward_propagation(X, parameters):
    W1, b1, W2, b2 = parameters['W1'], parameters['b1'], parameters['W2'], parameters['b2']
    Z1 = tf.matmul(W1, X) + b1
    A1 = tf.nn.tanh(Z1)
    Z2 = tf.matmul(W2, A1) + b2
    A2 = tf.nn.sigmoid(Z2)
    return {"Z1": Z1, "A1": A1, "Z2": Z2, "A2": A2}

# Compute the cross-entropy loss
def compute_loss(A2, Y):
    m = Y.shape[1]
    loss = -tf.reduce_mean(Y * tf.math.log(A2) + (1 - Y) * tf.math.log(1 - A2))
    return loss

# Backward propagation
def backward_propagation(X, Y, parameters, cache):
    m = X.shape[1]
    A1, A2 = cache['A1'], cache['A2']
    W2 = parameters['W2']
    dZ2 = A2 - Y
    dW2 = (1 / m) * tf.matmul(dZ2, tf.transpose(A1))
    db2 = (1 / m) * tf.reduce_sum(dZ2, axis=1, keepdims=True)
    dZ1 = tf.matmul(tf.transpose(W2), dZ2) * (1 - tf.square(A1))
    dW1 = (1 / m) * tf.matmul(dZ1, tf.transpose(X))
    db1 = (1 / m) * tf.reduce_sum(dZ1, axis=1, keepdims=True)
    return {"dW1": dW1, "db1": db1, "dW2": dW2, "db2": db2}

# Update parameters
def update_parameters(parameters, grads, learning_rate):
    parameters['W1'].assign_sub(learning_rate * grads['dW1'])
    parameters['b1'].assign_sub(learning_rate * grads['db1'])
    parameters['W2'].assign_sub(learning_rate * grads['dW2'])
    parameters['b2'].assign_sub(learning_rate * grads['db2'])
    return parameters

# Training the model
def train_model(X, Y, input_shape, hidden_units, learning_rate, num_epochs):
    parameters = initialize_parameters(input_shape, hidden_units)
    for epoch in range(num_epochs):
        with tf.GradientTape() as tape:
            cache = forward_propagation(X, parameters)
            loss = compute_loss(cache['A2'], Y)
        grads = tape.gradient(loss, parameters.values())
        grads = dict(zip(parameters.keys(), grads))
        parameters = update_parameters(parameters, grads, learning_rate)
        if epoch % 1000 == 0:
            print(f"Loss after epoch {epoch}: {loss.numpy():.4f}")
    return parameters

# Run the model
input_shape = (X.shape[0],)
hidden_units = 4
learning_rate = 1.5
num_epochs = 10000
trained_parameters = train_model(X, Y, input_shape, hidden_units, learning_rate, num_epochs)

# Predict function
def predict(X, parameters):
    cache = forward_propagation(X, parameters)
    predictions = tf.round(cache['A2'])
    return predictions.numpy()

# Plot the decision boundary
def plot_decision_boundary(X, Y, parameters):
    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1
    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),
                         np.arange(y_min, y_max, 0.01))
    Z = predict(np.c_[xx.ravel(), yy.ravel()].T, parameters)
    Z = Z.reshape(xx.shape)
    plt.contourf(xx, yy, Z, alpha=0.8)
    plt.scatter(X[0, :], X[1, :], c=Y, edgecolors='k', s=20)
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.title('Decision Boundary')
    plt.show()

# Plot decision boundary
plot_decision_boundary(X, Y, trained_parameters)
